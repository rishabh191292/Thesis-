{"cells":[{"metadata":{},"cell_type":"markdown","source":"****Environment List\n\n1. conda install pandas \n2. conda install numpy \n3. conda install statsmodels\n4. conda install scikit-learn \n5. conda install keras\n6. conda install tensor-flow\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib\nimport datetime as dt\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"jan_to_jun_2009 = pd.read_csv(\"../input/thesis/jan_to_jun_2009.csv\",index_col=0)\njul_to_dec_2009 = pd.read_csv(\"../input/thesis/jul_to_dec_2009.csv\",index_col=0)\njan_to_jun_2010 = pd.read_csv(\"../input/thesis/jan_to_aug_2010.csv\",index_col=0)\njul_to_dec_2010 = pd.read_csv(\"../input/thesis/sep_to_dec_2010.csv\",index_col=0)\njan_to_jun_2011 = pd.read_csv(\"../input/thesis/jan_to_jun_2011.csv\",index_col=0)\njul_to_dec_2011 = pd.read_csv(\"../input/thesis/jul_to_dec_2011.csv\",index_col=0)\njan_to_jun_2012 = pd.read_csv(\"../input/thesis/jan_to_jun_2012.csv\",index_col=0) \njul_to_dec_2012 = pd.read_csv(\"../input/thesis/jul_to_dec_2012.csv\",index_col=0)\njan_to_jun_2013 = pd.read_csv(\"../input/thesis/jan_to_jun_2013.csv\",index_col=0)\njul_to_dec_2013 = pd.read_csv(\"../input/thesis/jul_to_dec_2013.csv\",index_col=0)\njan_to_jun_2014 = pd.read_csv(\"../input/thesis/jan_to_jun_2014.csv\",index_col=0)\njul_to_dec_2014 = pd.read_csv(\"../input/thesis/jul_to_dec_2014.csv\",index_col=0)\njan_to_jun_2015 = pd.read_csv(\"../input/thesis/jan_to_jun_2015.csv\",index_col=0)\njul_to_dec_2015 = pd.read_csv(\"../input/thesis/jul_to_dec_2015.csv\",index_col=0)\njan_to_jul_2016 = pd.read_csv(\"../input/thesis/jan_to_jul_2016.csv\",index_col=0)\n\n#ALL Files are concatenated together \n\ndf = pd.concat([jan_to_jun_2009,jul_to_dec_2009,jan_to_jun_2010,jul_to_dec_2010,jan_to_jun_2011,jul_to_dec_2011,jan_to_jun_2012,jul_to_dec_2012,jan_to_jun_2013,jul_to_dec_2013,jan_to_jun_2014,jul_to_dec_2014,jan_to_jun_2015,jul_to_dec_2015,jan_to_jul_2016],axis=0)\n\n#Instrument type Equity is selected.\n\ndf = df.loc[df['RFDE_INSTR_TYPE'] == 'REG_DL_INSTR_EQ']\n\n#Renaming of the column \n\ndf = df.rename(columns={'VALUE (in Rs)': 'Sale'})\n\n#Converting the TR_DATE columns which denotes the transaction date into date time formate.\n\ndf['TR_DATE'] = df['TR_DATE'].astype('datetime64[D]')\n\ndf1 = pd.DataFrame()\ndf2 = pd.DataFrame()\n\ndf1['Date'] = df['TR_DATE']\ndf1['Sale'] = df['Sale']\ndf2['Date'] = df['TR_DATE']\ndf2['Inflation-Rate'] = df['Inflation-Rate']\ndf2['BSE_Close'] = df['BSE_Close']\ndf2['FDI-Inward'] = df['FDI-Inward']\ndf2['IIP'] = df['IIP']\ndf2['unemployment-rate'] = df['unemployment-rate']\ndf2['forex'] = df['foreign-exchange']\ndf2['GDP-Growth'] = df['GDP-Growth-Rate']\ndf2['FDI-Growth'] = df['FDI-Growth-Rate']\ndf2['twitter'] = df['twitter-sentiment']\n\n#Data is day wise distributed. Thus summing together to get the total sum of Equity instrument brought per day \n\ndf1 = df1.groupby(['Date']).sum()\n\ndf1 = df1.reset_index(level='Date')\n\n#Getting the exact value of different macro-economic variables per day. \n\ndf2 = df2.groupby(['Date'], as_index=False).mean()\n\n#Formulating the dataset with columns Date, Sale, and macro-economic variables. \n\ndf1['BSE_Close'] = df1['Date'].map(df2.set_index('Date')['BSE_Close'])\ndf1['FDI-Inward'] = df1['Date'].map(df2.set_index('Date')['FDI-Inward'])\n\ndf1['IIP'] = df1['Date'].map(df2.set_index('Date')['IIP'])\ndf1['forex'] = df1['Date'].map(df2.set_index('Date')['forex'])\ndf1['twitter'] = df1['Date'].map(df2.set_index('Date')['twitter'])\ndf1['U-R'] = df1['Date'].map(df2.set_index('Date')['unemployment-rate'])\ndf1['Inflation-Rate'] = df1['Date'].map(df2.set_index('Date')['Inflation-Rate'])\ndf1['GDP-Growth'] = df1['Date'].map(df2.set_index('Date')['GDP-Growth'])\ndf1['FDI-Growth'] = df1['Date'].map(df2.set_index('Date')['FDI-Growth'])\n\ntest = df1\ntest['U-R'] = test['U-R'].replace(to_replace=0, method='ffill')\ntest['FDI-Inward'] = test['FDI-Inward'].fillna(method='ffill')\n\n#For the year 2009 there are 3 dates for which twitter sentiment is missing. They are replaced by the previous values.\n\ntest['twitter'] = test['twitter'].replace(to_replace=-3.000000, method='ffill')\n\n\nabc = pd.DataFrame(data=test.values,columns=test.columns)\n\n#The column of Stock Sale is deleted and Column of Date is deleted, before passing it to the autoencoder \n\ndel test['Date']\ndel test['Sale']\ndata = []\ndata = test\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df['CUST']\ndel df['SE']\ndel df['Year-Month']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#GDP-Growth, Inflation-Rate, Unemployment-Rate are percentage values given in whole number formate.    \n\ndata['GDP-Growth'] = data['GDP-Growth'].div(100)\ndata['Inflation-Rate'] = data['Inflation-Rate'].div(100)\ndata['U-R'] = data['U-R'].div(100)\n\ndata_UR = data['U-R'].to_numpy()\ndata_UR = data_UR.reshape(len(data_UR),1)\ndata_Inflation_Rate = data['Inflation-Rate'].to_numpy()\ndata_Inflation_Rate = data_Inflation_Rate.reshape(len(data_Inflation_Rate),1)\ndata_GDP_Growth = data['GDP-Growth'].to_numpy()\ndata_GDP_Growth = data_GDP_Growth.reshape(len(data_GDP_Growth),1)\ndata_FDI_Growth = data['FDI-Growth'].to_numpy()\ndata_FDI_Growth = data_FDI_Growth.reshape(len(data_FDI_Growth),1)\n\ndata_forex = data.forex.values\ndata_forex = data_forex.reshape(len(data_forex),1)\ndata_IIP = data.IIP.values\ndata_IIP = data_IIP.reshape(len(data_forex),1)\ndata_FDI_Inward = data['FDI-Inward'].values\ndata_FDI_Inward = data_FDI_Inward.reshape(len(data_forex),1)\ndata_BSE_Close = data.BSE_Close.values\ndata_BSE_Close = data_BSE_Close.reshape(len(data_BSE_Close),1)\ndata_twitter = data['twitter'].to_numpy()\ndata_twitter = data_twitter.reshape(len(data_twitter),1)\n\n\n# BSE_Close, FDI_inward, IIP, foreign-exchange are normalized using minmax scaler \n\nscaler1 = MinMaxScaler(feature_range=(0, 1))\ndata_BSE_Close_normalize = scaler1.fit_transform(data_BSE_Close)\nscaler2 = MinMaxScaler(feature_range=(0, 1))\ndata_FDI_Inward_normalize = scaler2.fit_transform(data_FDI_Inward)\nscaler3 = MinMaxScaler(feature_range=(0, 1))\ndata_IIP_normalize = scaler3.fit_transform(data_IIP)\nscaler4 = MinMaxScaler(feature_range=(0, 1))\ndata_forex_normalize = scaler4.fit_transform(data_forex)\n#scaler5 = MinMaxScaler(feature_range=(0,1))\n#data_twitter_normalize = scaler5.fit_transform(data_twitter)\n\n#All of the normalized data plus percentage valued features are concatenated together \n\ndata_normalize = np.concatenate((data_BSE_Close_normalize,data_FDI_Inward_normalize,data_IIP_normalize,data_forex_normalize,data_twitter,data_UR,data_Inflation_Rate,data_GDP_Growth,data_FDI_Growth),axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\nfrom matplotlib import pyplot\nfrom keras.layers import Dropout\nimport tensorflow as tf\n# lstm autoencoder recreate sequence\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.utils import plot_model\nfrom keras.layers import LeakyReLU\nfrom keras.utils import plot_model\nfrom keras.layers import Input\nfrom keras.layers import concatenate\nfrom keras.layers import Dense\nfrom keras.layers.recurrent import LSTM\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = 19\nlr = 0.0001\nX_train = data_normalize.reshape((data_normalize.shape[0],1,data_normalize.shape[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Defining the input shape for the model  \n\nvisible1 = Input(shape=(1,9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Layer 1\n\nhidden1,state_h_1,state_c_1= LSTM(50,activation='tanh',input_shape=(1,9),return_sequences=True,return_state= True)(visible1)\n\n\nhidden_state_1 = Model(inputs = visible1,outputs=[hidden1,state_h_1,state_c_1])\n\n\n#Layer 2\n\nhidden2,state_h_2,state_c_2 = LSTM(1,activation='tanh',return_state=True)(hidden1)\n\n\nhidden_state_2 = Model(inputs = visible1,outputs=[hidden2,state_h_2,state_c_2])\n\n#Layer 3\n\nrepeatvector = RepeatVector(1, name=\"repeater\")(hidden2)\nrepearvector_state = Model(inputs=visible1,output=repeatvector)\n\n#Layer 4\n\nhidden3,state_h_3,state_c_3 = LSTM(50,activation='tanh', return_sequences=True,return_state=True)(repeatvector)\n\nhidden_state_3 = Model(inputs = visible1,outputs=[hidden3,state_h_3,state_c_3])\n\n\n\n#Layer 5\n\noutput = Dense(9)(hidden3)\n\nmodel = Model(inputs=visible1,outputs=output)\n\nadam = optimizers.Adam(lr)\n\nmodel.compile(loss='mse', optimizer=adam)\n\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#FITTING THE MODEL\n\nmodel.fit(X_train, X_train, epochs=200,batch_size=batch,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_state_1.save(\"LSTM-Autoencoder-Layer1.h5\")\nhidden_state_2.save(\"LSTM-Autoencoder-Layer2.h5\")\nhidden_state_3.save(\"LSTM-Autoencoder-Layer3.h5\")\nrepearvector_state.save(\"RepeatVector.h5\")\nmodel.save(\"LSTM-Autoencoder.h5\")\nprint(\"model save!!!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\n\nmodel = load_model('../input/lstm-weights/LSTM-Output.h5')\nhidden_state_1 = load_model('../input/lstm-weights/LSTM-Autoencoder-Layer1.h5')\nhidden_state_2 = load_model('../input/lstm-weights/LSTM-Autoencoder-Layer2.h5')\nhidden_state_3 = load_model('../input/lstm-weights/LSTM-Autoencoder-Layer3.h5')\nrepearvector_state = load_model('../input/lstm-weights/RepeatVector.h5')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LAYER 1 OUTPUT FROM THE LAYER ,HIDDEN STATE, AND THE CELL STATE ARE OUTPUTED  \n\nHidden_State_1_Output = hidden_state_1.predict(X_train)\nHidden_State_1_Output[1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Hidden_State_1_Output[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hidden_state_1_outut_return_sequence = Hidden_State_1_Output[0]\nhidden_state_1_outut_return_sequence.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LAYER 2 OUTPUT FROM THE LAYER ,HIDDEN STATE, AND THE CELL STATE ARE OUTPUTED \n\nHidden_State_2_Output = hidden_state_2.predict(X_train)\nhidden_state_2_outut_return_sequence = Hidden_State_2_Output[0]\nhidden_state_2_outut_return_sequence.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LAYER 3 OUTPUT FROM THE LAYER ,HIDDEN STATE, AND THE CELL STATE ARE OUTPUTED \n\nrepeat_vector_output = repearvector_state.predict(X_train)\nrepeat_vector_output.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LAYER 4 OUTPUT FROM THE LAYER ,HIDDEN STATE, AND THE CELL STATE ARE OUTPUTED \n\n#Reconstruction Layer \n\nhidden_state_3_outut_return_sequence = hidden_state_3.predict(X_train)\nhidden_state_3_outut_return_sequence[0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import plot_model\nplot_model(model, show_shapes=True, to_file='lstm_autoencoder.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = model.predict(X_train)\nyhat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicted Yhat reshape into original 2D array \n\nyhat = yhat.reshape(2052,9)\n\n#Storing individual columns to be rescaled back to original values \n\n#BSE_Close \narray1 = yhat[:,:1]\n\n#FDI-Inward\narray2 = yhat[:,1:2]\n\n#IIP\narray3 = yhat[:,2:3]\n\n#Foreign-Exchange \narray4 = yhat[:,3:4]\n\n#Twitter Sentiment Score \n#array5 = yhat[:,4:5]\n\narray1 = scaler1.inverse_transform(array1)\narray2 = scaler2.inverse_transform(array2)\narray3 = scaler3.inverse_transform(array3)\narray4 = scaler4.inverse_transform(array4)\n#array5 = scaler5.inverse_transform(array5)\n\n\n#Concatenated all the features together \n\ndata_autoencoder = np.concatenate((array1,array2,array3,array4,yhat[:,4:],),axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data after Autoencoder Operation \n\nDataAfterAutoencoder = pd.DataFrame(data=data_autoencoder)\nDataAfterAutoencoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Original Data \n\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}