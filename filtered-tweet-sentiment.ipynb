{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import preprocessor as p\n",
    "from preprocessor.api import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"new-tweet-9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        11/01/17\n",
       "1        11/01/17\n",
       "2        11/01/17\n",
       "3        11/01/17\n",
       "4        11/01/17\n",
       "           ...   \n",
       "37316    31/12/17\n",
       "37317    31/12/17\n",
       "37318    31/12/17\n",
       "37319    31/12/17\n",
       "37320    31/12/17\n",
       "Name: tweet-date, Length: 37321, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet-date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = []\n",
    "for i in range(0,len(df['tweet'])):\n",
    "    TextCleaning = clean(df['tweet'][i])\n",
    "    clean_text.append(TextCleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text = pd.DataFrame(clean_text)\n",
    "clean_text.columns=[\"Clean-Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    " \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "#after tweepy preprocessing the colon symbol left remain after      #removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    #remove hashtags \n",
    "    tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    \n",
    "    #remove url\n",
    "    tweet = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', tweet, flags=re.MULTILINE)\n",
    "    #filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "#looping through conditions\n",
    "    for w in word_tokens:\n",
    "#check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    #print(word_tokens)\n",
    "    #print(filtered_sentence)return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_2 = []\n",
    "for i in range(0,len(clean_text['Clean-Tweet'])):\n",
    "    TextCleaning = clean_tweets(df['tweet'][i])\n",
    "    clean_text_2.append(TextCleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_2 = pd.DataFrame(clean_text_2)\n",
    "clean_text_2.columns=[\"Clean-Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_2\n",
    "clean_text_2 = clean_text_2.drop_duplicates(subset=['Clean-Tweet'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_2=clean_text_2.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = []\n",
    "for i in range(0,len(clean_text_2['Clean-Tweet'])):\n",
    "    blob = TextBlob(clean_text_2['Clean-Tweet'][i])\n",
    "    Sentiment = blob.sentiment     \n",
    "    print(Sentiment)\n",
    "    polarity.append(Sentiment.polarity)\n",
    "    subjectivity = Sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(polarity)\n",
    "clean_text_2['Twitter-Sentiment'] = pd.DataFrame(polarity)\n",
    "df['BSE_Sentiment'] = clean_text_2['Twitter-Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"new_sentiment-9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet-date'] = df['tweet-date'].astype('datetime64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='tweet-date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new= df.groupby(['tweet-date']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BSE_Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet-date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11/01/17</th>\n",
       "      <td>0.070970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/03/17</th>\n",
       "      <td>0.117208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/04/17</th>\n",
       "      <td>0.133175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/05/17</th>\n",
       "      <td>0.051314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/07/17</th>\n",
       "      <td>0.106765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/08/17</th>\n",
       "      <td>0.018866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/09/17</th>\n",
       "      <td>-0.011064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/10/17</th>\n",
       "      <td>0.078236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/12/17</th>\n",
       "      <td>0.098539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/01/17</th>\n",
       "      <td>0.057181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/04/17</th>\n",
       "      <td>0.046460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/05/17</th>\n",
       "      <td>0.054136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/06/17</th>\n",
       "      <td>0.091730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/07/17</th>\n",
       "      <td>0.102446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/09/17</th>\n",
       "      <td>0.111560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/10/17</th>\n",
       "      <td>0.081555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/12/17</th>\n",
       "      <td>0.104721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/11/17</th>\n",
       "      <td>0.097410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13/12/17</th>\n",
       "      <td>0.113463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/11/17</th>\n",
       "      <td>0.105167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14/12/17</th>\n",
       "      <td>0.092948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/11/17</th>\n",
       "      <td>0.056950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15/12/17</th>\n",
       "      <td>0.122210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/11/17</th>\n",
       "      <td>0.092297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16/12/17</th>\n",
       "      <td>0.084635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/11/17</th>\n",
       "      <td>0.092815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17/12/17</th>\n",
       "      <td>0.085316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/11/17</th>\n",
       "      <td>0.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18/12/17</th>\n",
       "      <td>0.116696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/11/17</th>\n",
       "      <td>0.076826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19/12/17</th>\n",
       "      <td>0.121688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/11/17</th>\n",
       "      <td>0.101674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20/12/17</th>\n",
       "      <td>0.109443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/11/17</th>\n",
       "      <td>0.087031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21/12/17</th>\n",
       "      <td>0.104428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22/11/17</th>\n",
       "      <td>0.081255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22/12/17</th>\n",
       "      <td>0.153436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23/11/17</th>\n",
       "      <td>0.103392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23/12/17</th>\n",
       "      <td>0.081569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24/11/17</th>\n",
       "      <td>0.088712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24/12/17</th>\n",
       "      <td>0.148005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25/11/17</th>\n",
       "      <td>0.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25/12/17</th>\n",
       "      <td>0.108919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26/10/17</th>\n",
       "      <td>0.112722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26/11/17</th>\n",
       "      <td>0.084130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26/12/17</th>\n",
       "      <td>0.118138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/10/17</th>\n",
       "      <td>0.105942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/11/17</th>\n",
       "      <td>0.112666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27/12/17</th>\n",
       "      <td>0.112130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/11/17</th>\n",
       "      <td>0.075456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28/12/17</th>\n",
       "      <td>0.125910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/11/17</th>\n",
       "      <td>0.094316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29/12/17</th>\n",
       "      <td>0.148663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30/10/17</th>\n",
       "      <td>0.071728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30/11/17</th>\n",
       "      <td>0.103685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30/12/17</th>\n",
       "      <td>0.144429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31/10/17</th>\n",
       "      <td>0.076212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31/12/17</th>\n",
       "      <td>0.132019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            BSE_Sentiment\n",
       "tweet-date               \n",
       "11/01/17         0.070970\n",
       "11/03/17         0.117208\n",
       "11/04/17         0.133175\n",
       "11/05/17         0.051314\n",
       "11/07/17         0.106765\n",
       "11/08/17         0.018866\n",
       "11/09/17        -0.011064\n",
       "11/10/17         0.078236\n",
       "11/12/17         0.098539\n",
       "12/01/17         0.057181\n",
       "12/04/17         0.046460\n",
       "12/05/17         0.054136\n",
       "12/06/17         0.091730\n",
       "12/07/17         0.102446\n",
       "12/09/17         0.111560\n",
       "12/10/17         0.081555\n",
       "12/12/17         0.104721\n",
       "13/11/17         0.097410\n",
       "13/12/17         0.113463\n",
       "14/11/17         0.105167\n",
       "14/12/17         0.092948\n",
       "15/11/17         0.056950\n",
       "15/12/17         0.122210\n",
       "16/11/17         0.092297\n",
       "16/12/17         0.084635\n",
       "17/11/17         0.092815\n",
       "17/12/17         0.085316\n",
       "18/11/17         0.083300\n",
       "18/12/17         0.116696\n",
       "19/11/17         0.076826\n",
       "19/12/17         0.121688\n",
       "20/11/17         0.101674\n",
       "20/12/17         0.109443\n",
       "21/11/17         0.087031\n",
       "21/12/17         0.104428\n",
       "22/11/17         0.081255\n",
       "22/12/17         0.153436\n",
       "23/11/17         0.103392\n",
       "23/12/17         0.081569\n",
       "24/11/17         0.088712\n",
       "24/12/17         0.148005\n",
       "25/11/17         0.091100\n",
       "25/12/17         0.108919\n",
       "26/10/17         0.112722\n",
       "26/11/17         0.084130\n",
       "26/12/17         0.118138\n",
       "27/10/17         0.105942\n",
       "27/11/17         0.112666\n",
       "27/12/17         0.112130\n",
       "28/11/17         0.075456\n",
       "28/12/17         0.125910\n",
       "29/11/17         0.094316\n",
       "29/12/17         0.148663\n",
       "30/10/17         0.071728\n",
       "30/11/17         0.103685\n",
       "30/12/17         0.144429\n",
       "31/10/17         0.076212\n",
       "31/12/17         0.132019"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new.reset_index(level=['tweet-date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"final-sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
